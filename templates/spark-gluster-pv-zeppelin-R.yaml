kind: Template
apiVersion: v1
template: spark-pv-cluster
metadata:
  name: spark-pv-cluster
labels:
  app: spark-pv-cluster
objects:

##################################################################################
# Define the Gluster Cluster (see gluster-cluster.yaml)
##################################################################################
## setup gluster endpoints; this is the previously setup gluster cluster
# setup the Persistent Volumes that will be claimed by analytics
# note path is the name of the volume.

##################################################################################
# Volume Security
##################################################################################
#
# Accessing persistent storage requires coordination between the cluster and/or storage administrator
# and the end developer. The cluster administrator creates PVs, which abstract the underlying physical storage.
# The developer creates pods and, optionally, PVCs, which bind to PVs, based on matching criteria, such as capacity.
#
# Multiple persistent volume claims (PVCs) within the same project can bind to the same PV.
# However, once a PVC binds to a PV, that PV cannot be bound by a claim outside of the first claimâ€™s project.
# If the underlying storage needs to be accessed by multiple projects, then each project needs its own PV,
# which can point to the same physical storage. In this sense, a bound PV is tied to a project.
#
# For the cluster administrator, granting pods access to PVs involves:
#
#   knowing the group ID and/or user ID assigned to the actual storage,
#   understanding SELinux considerations, and
#   ensuring that these IDs are allowed in the range of legal IDs def[0, 185, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010]ined for the project
#     and/or the SCC that matches the requirements of the pod.
#
# Group IDs, the user ID, and SELinux values are defined in the SecurityContext section in a pod definition.
# Group IDs are global to the pod and apply to all containers defined in the pod.
# User IDs can also be global, or specific to each container.
#
# The supplementalGroups IDs are typically used for controlling access to shared storage, such as NFS and GlusterFS,
# whereas fsGroup is used for controlling access to block storage, such as Ceph RBD and iSCSI.
#
#           supplementalGroups: [0, 185, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010]
- kind: PersistentVolume
  apiVersion: v1
  metadata:
    name: ${MASTER_NAME}-scratch
    labels:
      name: ${MASTER_NAME}
      cluster: ${CLUSTER_NAME}
      gluster-cluster: glusterfs-cluster
  spec:
    capacity:
      storage: 2Gi
    accessModes:
      - ReadWriteMany
    glusterfs:
      endpoints: glusterfs-cluster
      path: /scratch
      readOnly: false
    persistentVolumeReclaimPolicy: Retain

- kind: PersistentVolume
  apiVersion: v1
  metadata:
    name: ${MASTER_NAME}-logs
    labels:
      name: ${MASTER_NAME}
      cluster: ${CLUSTER_NAME}
      gluster-cluster: glusterfs-cluster
  spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteMany
    glusterfs:
      endpoints: glusterfs-cluster
      path: /logs
      readOnly: false
    persistentVolumeReclaimPolicy: Retain

# mapped to PV by access mode
# mapped to "pod" by name in volumes section
- kind: PersistentVolumeClaim
  apiVersion: v1
  metadata:
    name: ${MASTER_NAME}-scratch
    labels:
      name: ${MASTER_NAME}
      cluster: ${CLUSTER_NAME}
      gluster-cluster: glusterfs-cluster
  spec:
    accessModes:
    - ReadWriteMany
    resources:
       requests:
         storage: 2Gi

- kind: PersistentVolumeClaim
  apiVersion: v1
  metadata:
    name: ${MASTER_NAME}-logs
    labels:
      name: ${MASTER_NAME}
      cluster: ${CLUSTER_NAME}
      gluster-cluster: glusterfs-cluster
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
       requests:
         storage: 5Gi

##################################################################################
# Define the Compute (Spark) Cluster
##################################################################################
- kind: Service
  apiVersion: v1
  metadata:
    name: ${MASTER_NAME}
    labels:
      name: ${MASTER_NAME}
  spec:
    ports:
      - protocol: TCP
        port: 7077
        targetPort: 7077
    selector:
      name: ${MASTER_NAME}

- kind: Service
  apiVersion: v1
  metadata:
    name: ${MASTER_NAME}-webui
    labels:
      name: ${MASTER_NAME}
  spec:
    ports:
      - protocol: TCP
        port: 8080
        targetPort: 8080
    selector:
      name: ${MASTER_NAME}

- kind: Service
  apiVersion: v1
  metadata:
    name: ${ZEPPELIN_NAME}
    labels:
      name: ${MASTER_NAME}
  spec:
    ports:
      - port: 80
        targetPort: 8080
    selector:
      name: ${ZEPPELIN_NAME}
    type: LoadBalancer

# Unsecure Route...  Need to research secure routes
#- kind: Route
#  apiVersion: v1
#  metadata:
#    name: ${ZEPPELIN_NAME}
#    labels:
#      name: ${MASTER_NAME}
#  spec:
#    to:
#      kind: Service
#      name: ${ZEPPELIN_NAME}
#    tls:
#      termination: edge
# 1) naming is local to this DeploymentConfig
# volumeMounts:
#  - name: glusterfs-data
#
# must match
#
# volumes:
#  - name: glusterfs-data
#
# 2) Access to gluster volumes
# determine ACL (group)
#    $ ls -lZ /mnt/glusterfs/
#    drwxrwx---. yarn hadoop system_u:object_r:fusefs_t:s0    HadoopVol
#
#    $ id yarn
#    uid=592(yarn) gid=590(hadoop) groups=590(hadoop)
#
#        securityContext:
#          supplementalGroups: [590]
# 3) privileged accounts
#            securityContext:
#              privileged: true
# requires a privileged account
# see https://adam.younglogic.com/2017/06/creating-a-privileged-container-in-openshift/
#
# oc create serviceaccount -n <my-namespace> <my-privilegeduser>
# oc adm policy add-scc-to-user privileged -n <my-namespace> -z <my-privilegeduser>
# add to template (pod...)
#    serviceAccountName:
#      privilegeduser
#
# or (dangerous)
#
# oc adm policy add-scc-to-useThe supplementalGroups IDs are typically used for controlling access to shared storage, such as NFS and GlusterFS, whereas fsGroup is used for controlling access to block storage, such as Ceph RBD and iSCSI.r privileged -n <my-namespace> -z default
- kind: DeploymentConfig
  apiVersion: v1
  metadata:
    name: ${MASTER_NAME}
    labels:
      cluster: ${CLUSTER_NAME}
      type: master
  spec:
    strategy:
      type: Rolling
    triggers:
      - type: ConfigChange
    replicas: 1
    selector:
      name: ${MASTER_NAME}
    template:
      metadata:
        labels:
          name: ${MASTER_NAME}
      spec:
        containers:
          - name: ${MASTER_NAME}
            image: ${SPARK_IMAGE}
            imagePullPolicy: "Always"
            env:
              - name: SPARK_MASTER_PORT
                value: "7077"
              - name: SPARK_MASTER_WEBUI_PORT
                value: "8080"
            ports:
              - containerPort: 7077
                protocol: TCP
              - containerPort: 8080
                protocol: TCP
            volumeMounts:
              - name: glusterfsdata
                mountPath: /mnt/data
              - name: glusterfslogs
                mountPath: /mnt/logs
        securityContext:
          supplementalGroups: [0]
        volumes:
          - name: glusterfsdata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}-scratch
          - name: glusterfslogs
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}-logs

- kind: DeploymentConfig
  apiVersion: v1
  metadata:
    name: ${WORKER_NAME}
    labels:
      cluster: ${CLUSTER_NAME}
      type: worker
  spec:
    strategy:
      type: Rolling
    triggers:
      - type: ConfigChange
    replicas: ${WORKER_REPLICAS}
    selector:
      name: ${WORKER_NAME}
    template:
      metadata:
        labels:
          name: ${WORKER_NAME}
      spec:
        containers:
          - name: ${WORKER_NAME}
            image: ${SPARK_IMAGE}
            imagePullPolicy: "Always"
            env:
              - name: SPARK_MASTER_ADDRESS
                value: spark://${MASTER_NAME}:7077
              - name: SPARK_MASTER_UI_ADDRESS
                value: http://${MASTER_NAME}-webui:8080
            volumeMounts:
              - name: glusterfsdata
                mountPath: /mnt/data
              - name: glusterfslogs
                mountPath: /mnt/logs
        securityContext:
          supplementalGroups: [0]
        volumes:
          - name: glusterfsdata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}-scratch
          - name: glusterfslogs
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}-logs

##################################################################################
# Define the Compute (Spark) Cluster
##################################################################################
- kind: Service
  apiVersion: v1
  metadata:
    name: ${ZEPPELIN_NAME}-spark-ui
    labels:
      name: ${MASTER_NAME}
  spec:
    ports:
      - port: 4040
        targetPort: 4040
    selector:
      name: ${ZEPPELIN_NAME}
    type: LoadBalancer

- kind: DeploymentConfig
  apiVersion: v1
  metadata:
    name: ${ZEPPELIN_NAME}
    labels:
      cluster: ${CLUSTER_NAME}
      type: api
  spec:
    strategy:
      type: Rolling
    triggers:
      - type: ConfigChange
    replicas: 0
    selector:
      name: ${ZEPPELIN_NAME}
    template:
      metadata:
        labels:
          name: ${ZEPPELIN_NAME}
      spec:
        containers:
          - name: ${ZEPPELIN_NAME}
            image: ${ZEPPELIN_IMAGE}
            imagePullPolicy: "Always"
            ports:
              - containerPort: 8080
                protocol: TCP
              - containerPort: 4040
                protocol: TCP
            env:
              - name: SPARK_MASTER_ADDRESS
                value: spark://${MASTER_NAME}:7077
              - name: MASTER
                value: spark://${MASTER_NAME}:7077
              - name: SPARK_MASTER_UI_ADDRESS
                value: http://${MASTER_NAME}-webui:8080
              - name: ZEPPELIN_SPARK_USEHIVECONTEXT
                value: "false"
              - name: ZEPPELIN_R_KNITR
                value: "false"
            volumeMounts:
              - name: glusterfsdata
                mountPath: /mnt/data
              - name: glusterfslogs
                mountPath: /mnt/logs
        securityContext:
          supplementalGroups: [0]
        volumes:
          - name: glusterfsdata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}-scratch
          - name: glusterfslogs
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}-logs

# R-Studio, if we define a  mount point, will make this the R_USER_LIBS.
# will then survive restarts (nop
- kind: Service
  apiVersion: v1
  metadata:
    name: ${RSTUDIO_NAME}
    labels:
      name: ${RSTUDIO_NAME}
      cluster: ${CLUSTER_NAME}
      type: api
  spec:
    ports:
      - protocol: TCP
        port: 8787
        targetPort: 8787
    selector:
      name: ${RSTUDIO_NAME}

- kind: DeploymentConfig
  apiVersion: v1
  metadata:
    name: ${RSTUDIO_NAME}
  spec:
    strategy:
      type: Rolling
    triggers:
      - type: ConfigChange
    replicas: 0
    selector:
      name: ${RSTUDIO_NAME}
    template:
      metadata:
        labels:
          name: ${RSTUDIO_NAME}
      spec:
        containers:
          - name: ${RSTUDIO_NAME}
            image: ${RSTUDIO_IMAGE}
            env:
              - name: SPARK_MASTER_ADDRESS
                value: spark://${MASTER_NAME}:7077
              - name: SPARK_MASTER_UI_ADDRESS
                value: http://${MASTER_NAME}-webui:8080
              - name: R_STUDIO_LIB_SITE
                value: /mnt/data
              - name: CRAN_REPO
                value: https://cran.rstudio.com/
              - name: BIO_REPO
                value: https://bioconductor.org/
            ports:
              - containerPort: 8787
                protocol: TCP
            volumeMounts:
              - name: glusterfsdata
                mountPath: /mnt/data
              - name: glusterfslogs
                mountPath: /mnt/logs
        securityContext:
          supplementalGroups: [0]
        volumes:
          - name: glusterfsdata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}-scratch
          - name: glusterfslogs
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}-logs
##################################################################################
# Parameter Block
##################################################################################
parameters:
- name: SPARK_IMAGE
  description: Name of the Spark master/worker image
  value: docker.io/jeffreymanning/spark-node:latest
- name: CLUSTER_NAME
  description: name of cluster - applicable to spark infrastructure
  generate: expression
  from: "cluster-[a-z0-9]{4}"
  required: true
- name: MASTER_NAME
  description: master name used as a service name and a selector
  generate: expression
  from: "spark-master-[a-z0-9]{4}"
  required: true
- name: WORKER_NAME
  description: worker name used as a selector
  generate: expression
  from: "spark-worker-[a-z0-9]{4}"
  required: true
- name: WORKER_REPLICAS
  description: worker replica count
  value: "1"
  required: true
# zeppelin parameters
- name: ZEPPELIN_NAME
  description: zeppelin name used for selector - combines with master name
  generate: expression
  from: "zeppelin-[a-z0-9]{4}"
  required: true
- name: ZEPPELIN_IMAGE
  description: Name of the Zeppelin interface
  value: docker.io/jeffreymanning/zeppelin-src:latest
###  R-studio installation
- name: RSTUDIO_IMAGE
  description: Name of the r-studio api image
  value: docker.io/jeffreymanning/r-studio:latest
- name: RSTUDIO_NAME
  description: rstudio name used as a selector
  generate: expression
  from: "rstudio-server-[a-z0-9]{4}"
  required: true